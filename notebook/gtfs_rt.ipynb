{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in static gtfs\n",
    "# gtfs static file needs to be unzipped into gtfs folder\n",
    "\n",
    "trips = pd.read_csv('./gtfs/trips.txt', low_memory=False)\n",
    "routes = pd.read_csv('./gtfs/routes.txt', low_memory=False)\n",
    "stops = pd.read_csv('./gtfs/stops.txt', low_memory=False)\n",
    "stop_times = pd.read_csv('./gtfs/stop_times.txt', low_memory=False, parse_dates=['arrival_time','departure_time'])\n",
    "\n",
    "stop_times_drop_columns = [\n",
    "    'stop_headsign',\n",
    "    'continuous_pickup',\n",
    "    'continuous_drop_off'\n",
    "]\n",
    "stop_times.drop(columns=stop_times_drop_columns, inplace=True)\n",
    "\n",
    "# create gtfs static headways\n",
    "\n",
    "trips = trips.merge(routes[['route_id','route_type']], how='left', on=['route_id'])\n",
    "stop_times = stop_times.merge(trips[['trip_id','route_type','route_id','service_id','direction_id']], how='left', on=['trip_id'])\n",
    "stop_times['stop_id_grouped'] = np.where(stop_times['checkpoint_id'].isna(), stop_times['stop_id'], stop_times['checkpoint_id'])\n",
    "\n",
    "sub_stop_times = stop_times[(stop_times.route_type < 2)]\n",
    "\n",
    "def time_to_seconds(time:str) -> int:\n",
    "    (hour, min, sec) = time.split(\":\")\n",
    "    return int(hour) * 3600 + int(min) * 60 + int(sec)\n",
    "\n",
    "sub_stop_times.loc[:,'departure_time_sec'] = sub_stop_times.loc[:,'departure_time'].apply(time_to_seconds)\n",
    "sub_stop_times = sub_stop_times.sort_values(by=['direction_id','route_id','service_id','stop_id_grouped','departure_time'])\n",
    "\n",
    "sub_stop_times['prev_departure_time_sec'] = sub_stop_times['departure_time_sec'].shift().where(sub_stop_times.stop_id_grouped.eq(sub_stop_times.stop_id_grouped.shift()))\n",
    "sub_stop_times.dropna(axis=0, subset=['prev_departure_time_sec'], inplace=True)\n",
    "sub_stop_times['prev_departure_time_sec'] = sub_stop_times['prev_departure_time_sec'].astype('int', errors='ignore')\n",
    "sub_stop_times['head_way'] = sub_stop_times['departure_time_sec'] - sub_stop_times['prev_departure_time_sec']\n",
    "sub_stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times[(stop_times.checkpoint_id.isna())][['stop_id','checkpoint_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "# pull down vehicle position file list from s3\n",
    "\n",
    "BUCKET_NAME = 'mbta-ctd-dataplatform-dev-springboard'\n",
    "FILE_LIST_FILE = pathlib.Path('gtfs_rt_vehicle_pos')\n",
    "PREFIX = 'lamp/RT_VEHICLE_POSITIONS/year=2022/month=7/day='\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "client = bucket.meta.client\n",
    "\n",
    "t0 = time.monotonic()\n",
    "\n",
    "if FILE_LIST_FILE.is_file():\n",
    "    with open(FILE_LIST_FILE, mode='rb') as f:\n",
    "        obj_list = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    obj_list = []\n",
    "    for prefix in ('20','21','22',):\n",
    "        paginator = client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Prefix=f'{PREFIX}{prefix}/',\n",
    "        )\n",
    "        for page in pages:\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Size'] > 0:\n",
    "                    obj_list.append(obj)\n",
    "    with open(FILE_LIST_FILE, mode='wb') as f:\n",
    "        pickle.dump(obj_list, f)\n",
    "\n",
    "run_time = time.monotonic() - t0\n",
    "\n",
    "print(f\"{len(obj_list):,} files found in {run_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import IO\n",
    "\n",
    "# pull down vehicle positions files from s3 and concat into dataframe\n",
    "\n",
    "def get_zip_buffer(filename: str) -> IO[bytes]:\n",
    "    \"\"\"\n",
    "    Get a buffer for a zip file from s3 so that it can be read by zipfile\n",
    "    module. filename is assumed to be the full path to the zip file without the\n",
    "    s3:// prefix. Return it along with the last modified date for this s3\n",
    "    object.\n",
    "    \"\"\"\n",
    "    # inspired by\n",
    "    # https://betterprogramming.pub/unzip-and-gzip-incoming-s3-files-with-aws-lambda-f7bccf0099c9\n",
    "    (bucket, file) = filename.split(\"/\", 1)\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    zipped_file = s3_resource.Object(bucket_name=bucket, key=file)\n",
    "\n",
    "    return io.BytesIO(zipped_file.get()[\"Body\"].read())\n",
    "\n",
    "\n",
    "drop_columns = [\n",
    "    'occupancy_percentage',\n",
    "    'occupancy_status',\n",
    "    'vehicle_id', # always same as 'entity_id'\n",
    "    # 'start_date', # redundant with timestamp\n",
    "    # 'vehicle_label',\n",
    "    'vehicle_consist',\n",
    "    'bearing',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'trip_id', # Always None?\n",
    "]\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "for obj in obj_list:\n",
    "    filename = f\"{BUCKET_NAME}/{obj['Key']}\"\n",
    "    print(f\"Downloading: {filename}\")\n",
    "    t_df = pd.read_parquet(get_zip_buffer(filename))\n",
    "    t_df.drop(columns=drop_columns, inplace=True)\n",
    "    t_df = t_df[(t_df.route_id.isin(sub_stop_times.route_id.unique()))]\n",
    "    if df is None:\n",
    "        df = t_df\n",
    "    else:\n",
    "        df = pd.concat([df,t_df])\n",
    "    print(f\"Processed: {filename}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_cols = ['entity_id','current_status','current_stop_sequence','stop_id','direction_id','route_id','schedule_relationship','start_date','start_time','vehicle_label']\n",
    "aggfunc = {'vehicle_timestamp':['min','max']}\n",
    "live_headways = df.pivot_table(df, index=group_by_cols, aggfunc=aggfunc).reset_index()\n",
    "\n",
    "live_headways.sort_values(by=['vehicle_label',('vehicle_timestamp','min')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = live_headways[(live_headways.current_status == 'STOPPED_AT')].sort_values(by=['vehicle_label',('vehicle_timestamp','min')])\n",
    "stops.columns = [col[0] if col[1] == '' else '_'.join(col).strip() for col in stops.columns.values]\n",
    "stops['next_stop_id'] = stops['stop_id'].shift(-1)\n",
    "stops = stops.merge(next_stop, how='left', on=['route_id','stop_id','direction_id'])\n",
    "\n",
    "stops['next_stop_ok'] = np.where((stops['expect_next_stop_id'].isna()) | (stops['next_stop_id'].isin(stops['expect_next_stop_id'])), True,False)\n",
    "# stops.groupby(by=['next_stop_ok']).size()\n",
    "stops[(stops.next_stop_ok == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
